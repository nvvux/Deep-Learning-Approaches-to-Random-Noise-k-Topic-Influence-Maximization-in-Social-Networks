{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T19:33:20.041947Z",
     "start_time": "2025-05-23T19:33:19.795373Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số nút: 4,039  |  Số cạnh: 88,234\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def load_facebook_graph(path=\"facebook\"):\n",
    "    G = nx.Graph()\n",
    "    with open(path, \"r\") as f:\n",
    "        # Đọc dòng đầu: có hai số (số nút, số cạnh×2)\n",
    "        header = f.readline().strip().split()\n",
    "        # Nếu đúng là hai số, ta bỏ qua; còn nếu không phải thì dùng nó làm cạnh\n",
    "        if len(header) == 2 and header[0].isdigit() and header[1].isdigit():\n",
    "            n_nodes, twice_edges = map(int, header)\n",
    "        else:\n",
    "            # coi là dòng cạnh đầu\n",
    "            u, v = map(int, header)\n",
    "            G.add_edge(u, v)\n",
    "\n",
    "        # Đọc phần còn lại, mỗi dòng một cạnh u v\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            u, v = map(int, parts[:2])\n",
    "            G.add_edge(u, v)\n",
    "    return G\n",
    "\n",
    "# Ví dụ chạy ngay\n",
    "G = load_facebook_graph(\"facebook\")\n",
    "print(f\"Số nút: {G.number_of_nodes():,}  |  Số cạnh: {G.number_of_edges():,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6f4323311b9c03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T19:31:01.544953Z",
     "start_time": "2025-05-23T19:29:59.699512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.4.2)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/212.5 MB 8.3 MB/s eta 0:00:26\n",
      "   ---------------------------------------- 1.8/212.5 MB 8.4 MB/s eta 0:00:26\n",
      "    --------------------------------------- 3.7/212.5 MB 7.0 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 5.5/212.5 MB 7.8 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 7.3/212.5 MB 8.1 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 9.2/212.5 MB 8.3 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 11.3/212.5 MB 8.5 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 13.1/212.5 MB 8.7 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 15.2/212.5 MB 8.8 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 17.0/212.5 MB 8.9 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 19.1/212.5 MB 9.0 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 21.2/212.5 MB 9.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 23.3/212.5 MB 9.2 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 25.4/212.5 MB 9.3 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 27.5/212.5 MB 9.4 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 29.6/212.5 MB 9.4 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 31.7/212.5 MB 9.5 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 33.8/212.5 MB 9.5 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 35.9/212.5 MB 9.6 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 38.0/212.5 MB 9.7 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 40.1/212.5 MB 9.7 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 42.2/212.5 MB 9.7 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 44.3/212.5 MB 9.8 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 46.4/212.5 MB 9.8 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 48.5/212.5 MB 9.8 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 50.6/212.5 MB 9.9 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 53.0/212.5 MB 9.9 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 55.1/212.5 MB 9.9 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 57.1/212.5 MB 9.9 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 59.2/212.5 MB 10.0 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 61.3/212.5 MB 10.0 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 63.4/212.5 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 65.5/212.5 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 67.9/212.5 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 70.0/212.5 MB 10.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 72.1/212.5 MB 10.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 74.4/212.5 MB 10.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 76.5/212.5 MB 10.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 78.6/212.5 MB 10.1 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 81.0/212.5 MB 10.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 83.1/212.5 MB 10.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 85.5/212.5 MB 10.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 87.6/212.5 MB 10.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 89.9/212.5 MB 10.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 92.3/212.5 MB 10.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 94.4/212.5 MB 10.3 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 96.7/212.5 MB 10.3 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 99.1/212.5 MB 10.4 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 101.4/212.5 MB 10.4 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 103.8/212.5 MB 10.4 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 106.2/212.5 MB 10.4 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 108.0/212.5 MB 10.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 109.8/212.5 MB 10.4 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 111.4/212.5 MB 10.3 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 113.2/212.5 MB 10.3 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 115.3/212.5 MB 10.3 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 117.2/212.5 MB 10.3 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 119.3/212.5 MB 10.3 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 121.1/212.5 MB 10.3 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 123.2/212.5 MB 10.3 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 125.3/212.5 MB 10.3 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 127.1/212.5 MB 10.3 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 129.2/212.5 MB 10.3 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 131.6/212.5 MB 10.3 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 133.7/212.5 MB 10.3 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 136.1/212.5 MB 10.3 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 138.1/212.5 MB 10.3 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 140.5/212.5 MB 10.3 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 142.9/212.5 MB 10.3 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 145.0/212.5 MB 10.3 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 147.3/212.5 MB 10.4 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 149.7/212.5 MB 10.4 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 152.0/212.5 MB 10.4 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 154.4/212.5 MB 10.4 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 156.8/212.5 MB 10.4 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 158.9/212.5 MB 10.4 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 160.4/212.5 MB 10.4 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 162.3/212.5 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 163.8/212.5 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 165.7/212.5 MB 10.3 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 167.5/212.5 MB 10.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 169.3/212.5 MB 10.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 171.2/212.5 MB 10.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 173.0/212.5 MB 10.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 175.1/212.5 MB 10.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 176.9/212.5 MB 10.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 178.8/212.5 MB 10.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 180.9/212.5 MB 10.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 182.7/212.5 MB 10.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 184.8/212.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 186.6/212.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 188.7/212.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 190.8/212.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 192.7/212.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 194.8/212.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 196.9/212.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 198.7/212.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 200.8/212.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 202.9/212.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 205.0/212.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 206.8/212.5 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  208.9/212.5 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  210.8/212.5 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 212.5/212.5 MB 10.1 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 8.9 MB/s eta 0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "\n",
      "  Attempting uninstall: sympy\n",
      "\n",
      "    Found existing installation: sympy 1.13.2\n",
      "\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "    Uninstalling sympy-1.13.2:\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ---------------------------------------- 4/4 [torchaudio]\n",
      "\n",
      "Successfully installed sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install networkx torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f170dc8d4000058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T19:45:12.996888Z",
     "start_time": "2025-05-23T19:33:25.696190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 4411413.3040\n",
      "Epoch 2/20, Loss: 4408861.8200\n",
      "Epoch 3/20, Loss: 4405109.6320\n",
      "Epoch 4/20, Loss: 4399414.8880\n",
      "Epoch 5/20, Loss: 4390926.0400\n",
      "Epoch 6/20, Loss: 4378804.8320\n",
      "Epoch 7/20, Loss: 4361847.2480\n",
      "Epoch 8/20, Loss: 4338994.4520\n",
      "Epoch 9/20, Loss: 4309517.7100\n",
      "Epoch 10/20, Loss: 4272101.8200\n",
      "Epoch 11/20, Loss: 4226191.6960\n",
      "Epoch 12/20, Loss: 4170607.9520\n",
      "Epoch 13/20, Loss: 4104160.5000\n",
      "Epoch 14/20, Loss: 4024498.4920\n",
      "Epoch 15/20, Loss: 3932422.5440\n",
      "Epoch 16/20, Loss: 3827764.0000\n",
      "Epoch 17/20, Loss: 3709595.5680\n",
      "Epoch 18/20, Loss: 3579219.7760\n",
      "Epoch 19/20, Loss: 3437007.2480\n",
      "Epoch 20/20, Loss: 3282201.5960\n",
      "Optimal seed sets: [{107, 1684}, {107, 1684}]\n",
      "Predicted spread per topic: [355.09832763671875, 435.040771484375]\n",
      "Total predicted spread: 790.1390991210938\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- Multi-Topic Independent Cascade Simulation ---\n",
    "class MultiTopicIC:\n",
    "    def __init__(self, G: nx.DiGraph, k: int,\n",
    "                 prob: Dict[int, Dict[Tuple[int, int], float]]):\n",
    "        self.G = G\n",
    "        self.k = k\n",
    "        self.prob = prob\n",
    "\n",
    "    def simulate(self, seeds: List[Set[int]]) -> List[Set[int]]:\n",
    "        activated = [set(S) for S in seeds]\n",
    "        frontier = [set(S) for S in seeds]\n",
    "        while any(frontier):\n",
    "            new_frontier = [set() for _ in range(self.k)]\n",
    "            for i in range(self.k):\n",
    "                for u in frontier[i]:\n",
    "                    for v in self.G.successors(u):\n",
    "                        if v not in activated[i] and random.random() <= self.prob[i].get((u, v), 0.0):\n",
    "                            new_frontier[i].add(v)\n",
    "                            activated[i].add(v)\n",
    "            frontier = new_frontier\n",
    "        return activated\n",
    "\n",
    "    def expected_spread(self, seeds: List[Set[int]], runs: int = 100) -> List[float]:\n",
    "        total = [0.0] * self.k\n",
    "        for _ in range(runs):\n",
    "            act = self.simulate(seeds)\n",
    "            for i in range(self.k):\n",
    "                total[i] += len(act[i])\n",
    "        return [t / runs for t in total]\n",
    "\n",
    "# --- Probability Generator ---\n",
    "def generate_probabilities(G: nx.DiGraph, k: int, sigma: float = 0.1) -> Dict[int, Dict[Tuple[int, int], float]]:\n",
    "    indeg = {v: G.in_degree(v) if G.in_degree(v) > 0 else 1 for v in G.nodes()}\n",
    "    probs: Dict[int, Dict[Tuple[int, int], float]] = {i: {} for i in range(k)}\n",
    "    for i in range(k):\n",
    "        for u, v in G.edges():\n",
    "            base_p = 1.0 / indeg[v]\n",
    "            noise = random.gauss(0, sigma)\n",
    "            p = min(max(base_p + noise, 0.0), 1.0)\n",
    "            probs[i][(u, v)] = p\n",
    "    return probs\n",
    "\n",
    "# --- MLP Model for Influence Prediction ---\n",
    "class InfluenceMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [64, 32]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# --- Feature Extraction ---\n",
    "def extract_features(seed_set: Set[int], degree_centrality: Dict[int, float], topic: int) -> torch.Tensor:\n",
    "    sum_deg = sum(degree_centrality.get(v, 0.0) for v in seed_set)\n",
    "    size = len(seed_set)\n",
    "    return torch.tensor([sum_deg, size, float(topic)], dtype=torch.float32)\n",
    "\n",
    "# --- Greedy Search for Optimal Seed Sets ---\n",
    "def search_optimal_seeds(model: InfluenceMLP,\n",
    "                         k: int,\n",
    "                         G: nx.DiGraph,\n",
    "                         degree_centrality: Dict[int, float],\n",
    "                         budget: List[int]) -> List[Set[int]]:\n",
    "    seeds = [set() for _ in range(k)]\n",
    "    nodes = set(G.nodes())\n",
    "    for i in range(k):\n",
    "        for _ in range(budget[i]):\n",
    "            best_node, best_gain = None, -float('inf')\n",
    "            current_feat = extract_features(seeds[i], degree_centrality, i)\n",
    "            current_pred = model(current_feat.unsqueeze(0))[0].item()\n",
    "            for v in nodes - seeds[i]:\n",
    "                feat = extract_features(seeds[i] | {v}, degree_centrality, i)\n",
    "                pred = model(feat.unsqueeze(0))[0].item()\n",
    "                gain = pred - current_pred\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_node = gain, v\n",
    "            if best_node is None:\n",
    "                break\n",
    "            seeds[i].add(best_node)\n",
    "    return seeds\n",
    "\n",
    "# --- Main Workflow ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load graph\n",
    "    path = \"facebook\"\n",
    "    G_undirected = nx.read_edgelist(path, nodetype=int)\n",
    "    G = G_undirected.to_directed()\n",
    "\n",
    "    # Generate topic-specific probabilities\n",
    "    k = 2\n",
    "    sigma_noise = 0.1\n",
    "    prob = generate_probabilities(G, k, sigma_noise)\n",
    "    ic_model = MultiTopicIC(G, k, prob)\n",
    "\n",
    "    # Precompute degree centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "    # Prepare training data by sampling random seed sets\n",
    "    budget = [2] * k  # number of seeds per topic\n",
    "    num_samples = 500\n",
    "    X_list, y_list = [], []\n",
    "    for topic in range(k):\n",
    "        for _ in range(num_samples):\n",
    "            S = set(random.sample(list(G.nodes()), budget[topic]))\n",
    "            feat = extract_features(S, degree_centrality, topic)\n",
    "            spread = ic_model.expected_spread([S if t == topic else set() for t in range(k)], runs=50)[topic]\n",
    "            X_list.append(feat)\n",
    "            y_list.append(spread)\n",
    "\n",
    "    # Build DataLoader\n",
    "    X = torch.stack(X_list)\n",
    "    y = torch.tensor(y_list, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Initialize and train MLP\n",
    "    model = InfluenceMLP(input_dim=3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    epochs = 20\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataset):.4f}\")\n",
    "\n",
    "    # Find optimal seed sets using trained MLP\n",
    "    optimal_seeds = search_optimal_seeds(model, k, G, degree_centrality, budget)\n",
    "    print(\"Optimal seed sets:\", optimal_seeds)\n",
    "\n",
    "    # Compute predicted spread\n",
    "    spreads = [model(extract_features(optimal_seeds[i], degree_centrality, i).unsqueeze(0))[0].item() for i in range(k)]\n",
    "    total_spread = sum(spreads)\n",
    "    print(\"Predicted spread per topic:\", spreads)\n",
    "    print(\"Total predicted spread:\", total_spread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193f950f8a8b9140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T20:28:58.036153Z",
     "start_time": "2025-05-23T20:04:29.004046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6486516.7000\n",
      "Epoch 2 Loss: 6478378.0356\n",
      "Epoch 3 Loss: 6466759.3778\n",
      "Epoch 4 Loss: 6449385.6711\n",
      "Epoch 5 Loss: 6423914.3044\n",
      "Epoch 6 Loss: 6387855.6933\n",
      "Epoch 7 Loss: 6338559.2889\n",
      "Epoch 8 Loss: 6273408.6000\n",
      "Epoch 9 Loss: 6189761.4089\n",
      "Epoch 10 Loss: 6085232.7978\n",
      "Epoch 11 Loss: 5957884.2756\n",
      "Epoch 12 Loss: 5805949.9178\n",
      "Epoch 13 Loss: 5627688.9378\n",
      "Epoch 14 Loss: 5423278.3978\n",
      "Epoch 15 Loss: 5192461.2889\n",
      "Epoch 16 Loss: 4935960.1333\n",
      "Epoch 17 Loss: 4656010.9867\n",
      "Epoch 18 Loss: 4353101.7178\n",
      "Epoch 19 Loss: 4030545.3278\n",
      "Epoch 20 Loss: 3691706.8778\n",
      "Epoch 21 Loss: 3344061.7822\n",
      "Epoch 22 Loss: 2991862.0200\n",
      "Epoch 23 Loss: 2641128.8200\n",
      "Epoch 24 Loss: 2296772.7489\n",
      "Epoch 25 Loss: 1966447.0989\n",
      "Optimal seed sets: [{0, 107, 3437, 1684, 1912}, {1888, 1800, 2347, 2543, 1663}, {1730, 483, 1352, 2266, 348}]\n",
      "Final expected spread per topic: [2105.38, 355.72, 352.53]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- 1. Multi-Topic Independent Cascade with Gaussian Noise ---\n",
    "class MultiTopicIC:\n",
    "    def __init__(self,\n",
    "                 G: nx.DiGraph,\n",
    "                 k: int,\n",
    "                 base_prob: Dict[int, Dict[Tuple[int, int], float]],\n",
    "                 sigma: float = 0.1):\n",
    "        self.G = G\n",
    "        self.k = k\n",
    "        self.base_prob = base_prob\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def _noisy_prob(self, i: int, u: int, v: int) -> float:\n",
    "        p0 = self.base_prob[i].get((u, v), 0.0)\n",
    "        return float(min(max(p0 + random.gauss(0, self.sigma), 0.0), 1.0))\n",
    "\n",
    "    def simulate(self, seeds: List[Set[int]]) -> List[Set[int]]:\n",
    "        # giữ non-overlap: nếu node active topic A, không active B\n",
    "        activated = [set(S) for S in seeds]\n",
    "        frontier = [set(S) for S in seeds]\n",
    "\n",
    "        # track globally active to enforce competition\n",
    "        global_active: Dict[int, int] = {v: -1 for v in self.G.nodes()}\n",
    "        for i, S in enumerate(seeds):\n",
    "            for v in S:\n",
    "                global_active[v] = i\n",
    "\n",
    "        while any(frontier):\n",
    "            new_frontier = [set() for _ in range(self.k)]\n",
    "            for i in range(self.k):\n",
    "                for u in frontier[i]:\n",
    "                    for v in self.G.successors(u):\n",
    "                        if global_active[v] == -1 and random.random() <= self._noisy_prob(i, u, v):\n",
    "                            global_active[v] = i\n",
    "                            new_frontier[i].add(v)\n",
    "                            activated[i].add(v)\n",
    "            frontier = new_frontier\n",
    "        return activated\n",
    "\n",
    "    def expected_spread(self, seeds: List[Set[int]], runs: int = 100) -> List[float]:\n",
    "        cum = [0.0] * self.k\n",
    "        for _ in range(runs):\n",
    "            act = self.simulate(seeds)\n",
    "            for i in range(self.k):\n",
    "                cum[i] += len(act[i])\n",
    "        return [x / runs for x in cum]\n",
    "\n",
    "\n",
    "# --- 2. Generate base probabilities per topic ---\n",
    "def generate_base_probs(G: nx.DiGraph, k: int) -> Dict[int, Dict[Tuple[int, int], float]]:\n",
    "    # sử dụng 1 / indegree làm base\n",
    "    indeg = {v: max(G.in_degree(v), 1) for v in G.nodes()}\n",
    "    probs: Dict[int, Dict[Tuple[int, int], float]] = {i: {} for i in range(k)}\n",
    "    for i in range(k):\n",
    "        for u, v in G.edges():\n",
    "            probs[i][(u, v)] = 1.0 / indeg[v]\n",
    "    return probs\n",
    "\n",
    "\n",
    "# --- 3. MLP for influence prediction ---\n",
    "class InfluenceMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [64, 32]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(prev, h), nn.ReLU()]\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# --- 4. Feature extraction per (seed_set, topic) ---\n",
    "def extract_feature(seed_set: Set[int],\n",
    "                    degree_centrality: Dict[int, float],\n",
    "                    topic: int) -> torch.Tensor:\n",
    "    # ví dụ: [|S|, sum_deg, topic_id]\n",
    "    return torch.tensor([\n",
    "        float(len(seed_set)),\n",
    "        sum(degree_centrality[v] for v in seed_set),\n",
    "        float(topic)\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# --- 5. Greedy search ensure non-overlap & per-topic budget ---\n",
    "def search_optimal_seeds(\n",
    "    model: InfluenceMLP,\n",
    "    G: nx.DiGraph,\n",
    "    degree_centrality: Dict[int, float],\n",
    "    k: int,\n",
    "    budget_per_topic: List[int]\n",
    ") -> List[Set[int]]:\n",
    "    chosen: List[Set[int]] = [set() for _ in range(k)]\n",
    "    available = set(G.nodes())\n",
    "\n",
    "    for i in range(k):\n",
    "        for _ in range(budget_per_topic[i]):\n",
    "            best_node, best_gain = None, -1e9\n",
    "            base_feat = extract_feature(chosen[i], degree_centrality, i).unsqueeze(0)\n",
    "            base_pred = model(base_feat).item()\n",
    "            for v in available - chosen[i]:\n",
    "                feat = extract_feature(chosen[i] | {v}, degree_centrality, i).unsqueeze(0)\n",
    "                gain = model(feat).item() - base_pred\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_node = gain, v\n",
    "            if best_node is None:\n",
    "                break\n",
    "            chosen[i].add(best_node)\n",
    "            available.remove(best_node)  # enforce non-overlap\n",
    "    return chosen\n",
    "\n",
    "\n",
    "# --- 6. Main training & search flow ---\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"facebook\"\n",
    "    G_undirected = nx.read_edgelist(path, nodetype=int)\n",
    "    G = G_undirected.to_directed()\n",
    "    # 2. prepare IC model\n",
    "    k = 3\n",
    "    base_prob = generate_base_probs(G, k)\n",
    "    ic = MultiTopicIC(G, k, base_prob, sigma=0.1)\n",
    "\n",
    "    # 3. degree centrality\n",
    "    deg_c = nx.degree_centrality(G)\n",
    "\n",
    "    # 4. generate training samples\n",
    "    budget = [5] * k\n",
    "    samples = []\n",
    "    targets = []\n",
    "    for topic in range(k):\n",
    "        for _ in range(300):\n",
    "            S = set(random.sample(list(G.nodes()), budget[topic]))\n",
    "            # chỉ tính spread cho đúng topic, các topic khác empty\n",
    "            spreads = ic.expected_spread([S if t == topic else set() for t in range(k)], runs=30)\n",
    "            samples.append(extract_feature(S, deg_c, topic))\n",
    "            targets.append(spreads[topic])\n",
    "\n",
    "    X = torch.stack(samples)       # [N, 3]\n",
    "    y = torch.tensor(targets)      # [N]\n",
    "    ds = TensorDataset(X, y)\n",
    "    loader = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "    # 5. init & train MLP\n",
    "    model = InfluenceMLP(input_dim=3)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(25):\n",
    "        epoch_loss = 0\n",
    "        for xb, yb in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(ds):.4f}\")\n",
    "\n",
    "    # 6. search optimal seeds\n",
    "    optimal = search_optimal_seeds(model, G, deg_c, k, budget)\n",
    "    print(\"Optimal seed sets:\", optimal)\n",
    "    # 7. estimate final spread\n",
    "    final = ic.expected_spread(optimal, runs=100)\n",
    "    print(\"Final expected spread per topic:\", final)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "99eaee7edab82b56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T20:38:32.958095Z",
     "start_time": "2025-05-23T20:38:32.954669Z"
    }
   },
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
